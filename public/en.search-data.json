{"/bigquery-utils/":{"data":{"":"","features#Features":"","next#Next":"üëã Welcome to the documentation of bigquery-advanced-utils!\nWhat is bigquery-advanced-utils? bigquery-advanced-utils is a Python package designed to enhance and extend Google‚Äôs official BigQuery client. It provides advanced features and utilities to tackle complex or uncommon challenges, helping developers avoid reinventing the wheel. As an open-source project, it is open to everyone and actively encourages contributions to make it more versatile and effective.\nFeatures Advanced Functionality - Solve complex BigQuery tasks with ease using specialized tools and utilities. Customizable and Extensible - Built for developers, the package supports flexible configurations and encourages contributions. Efficient Workflow - Save time with automated solutions for repetitive or challenging tasks. Open Source - Developed with transparency and collaboration in mind. Contributions are always welcome! Detailed Documentation - Easy-to-follow guides and examples to get you started quickly. Why Use bigquery-advanced-utils? This package enhances the standard BigQuery client by adding solutions for tasks that are not easily achievable with the base implementation. It reduces the effort required to handle complex scenarios, enabling developers to focus on solving business problems without being hindered by technical limitations.\nQuestions or Feedback? ‚ùì This project is maintained by Alessio Siciliano and is open to contributions. Have a question or feedback? Feel free to open an issue! Next Explore the following sections to get started:\nGetting StartedLearn how to use bigquery-advanced-utils in your projects","questions-or-feedback#Questions or Feedback?":"","what-is-bigquery-advanced-utils#What is \u003ccode\u003ebigquery-advanced-utils\u003c/code\u003e?":"","why-use-bigquery-advanced-utils#Why Use \u003ccode\u003ebigquery-advanced-utils\u003c/code\u003e?":""},"title":"BigQuery Advanced Utils"},"/bigquery-utils/getting-started/":{"data":{"":"","quick-start#Quick Start":"","start-as-new-project#Start as New Project":"Quick Start Alessio-Siciliano/bigquery-advanced-utils\nYou can quickly get started by installing the package with pip:\npip install bigquery-advanced-utils Start as New Project Prerequisites Before starting, you need to have the following software installed:\nPython Git (optional) Steps Create a new virtual environment python -m venv venv Activate the new environment source venv/bin/activate # on macOS/Linux venv\\Scripts\\activate # on Windows Install the package Install the latest version available with the package manager PIP:\npip install bigquery-advanced-utils Import classes in your code Now, you can import all the classes from ‚Äúbigquery_advanced_utils‚Äù as:\nfrom bigquery_advanced_utils.bigquery import BigQueryClient How to update the package with the future versions? To update the package in your project to its latest versions, run the following command:\npip install --upgrade bigquery-advanced-utils "},"title":"Getting Started"},"/bigquery-utils/modules/":{"data":{"":"Explore the following sections to learn how to use Hextra:\nOrganize Files\rConfiguration\rMarkdown\rSyntax Highlighting\rLaTeX\rDiagrams\rShortcodes\rDeploy Site"},"title":"Modules"},"/bigquery-utils/modules/bigquery/":{"data":{"":"","class-initialization#Class Initialization":"","introduction#Introduction":"","methods-overview#Methods Overview":"Introduction The BigQueryClient class is a custom extension of the Google Cloud BigQuery Client. This enhanced client provides additional functionality for managing BigQuery operations more efficiently. It includes built-in methods for managing permissions, handling datasets and tables, and automating common tasks such as loading data, exporting tables, and simulating queries.\nClass Initialization This constructor initializes the BigQueryClient class, which inherits from the google.cloud.bigquery.Client. It shares the attributes and behavior of the original BigQuery client while enabling customized extensions for easier BigQuery operations.\nParameters project_id (str): The project ID associated with the BigQuery client. credentials (Optional[Credentials]): The credentials used to authenticate the client. _http (Optional[requests.Session]): The HTTP session to use for requests. location (Optional[str]): Default geographic location for queries and jobs. default_query_job_config (Optional[QueryJobConfig]): Default configuration for query jobs. default_load_job_config (Optional[LoadJobConfig]): Default configuration for load jobs. client_info (Optional[ClientInfo]): Information about the client. client_options (Optional[Union[ClientOptions, Dict]]): Additional client options. Example BigQueryClient( project_id: str, credentials: Optional[Credentials] = None, _http: Optional[requests.Session] = None, location: Optional[str] = None, default_query_job_config: Optional[QueryJobConfig] = None, default_load_job_config: Optional[LoadJobConfig] = None, client_info: Optional[ClientInfo] = None, client_options: Optional[Union[ClientOptions, Dict]] = None, ) Methods Overview check_table_existence Description Checks whether a specific table exists within a given dataset.\nParameters dataset_id (str): The ID of the dataset to check. table_id (str): The ID of the table to check. Example exists = client.check_table_existence(\"my_dataset\", \"my_table\") print(f\"Table exists: {exists}\") load_data_from_csv Description Loads data from a CSV file into a specified BigQuery table. Optionally applies validation tests (custom or not) to the data.\nParameters dataset_id (str): The destination dataset ID. table_id (str): The destination table ID. csv_file_path (str): Path to the CSV file. test_functions (Optional[list]): A list of validation functions applied to the data. For the full list of predefined test (See the Custom checks class page for more details.) encoding (str): The file encoding (default is UTF-8). Example from functools import partial def custom_test(index, row, header, column_sums): if len(row) != len(header): raise ValueError(f\"Row {index} does not match header length.\") data_checks = CustomDataChecks() client.load_data_from_csv( dataset_id=\"my_dataset\", table_id=\"my_table\", csv_file_path=\"data.csv\", test_functions=[ custom_test, partial(data_checks.check_columns), partial( data_checks.check_unique, columns_to_test=[\"col1\"], ), ... ] ) simulate_query Description Simulates a query execution to estimate resource usage and provides metadata such as schema and referenced tables.\nParameters query (str): The SQL query to simulate. Returns dict: Metadata about the query execution, including schema, referenced tables, and bytes processed. Example simulation = client.simulate_query(\"SELECT * FROM `my_project.my_dataset.my_table` LIMIT 10\") print(simulation) grant_permission Description Manages permissions (add, remove, or update) for users on a BigQuery table or dataset.\nParameters resource_id (str): The resource ID in the format project_id.dataset_id (if dataset) or project_id.dataset_id.table_id (if table). user_permissions (List[Dict[str, str]]): A list of dictionaries containing user_email and role. action (str): The action to perform (add, remove, or update). Example permissions = [ {\"user_email\": \"user@example.com\", \"role\": \"READER\"}, {\"user_email\": \"admin@example.com\", \"role\": \"OWNER\"} ] client.grant_permission( resource_id=\"my_project.my_dataset\", user_permissions=permissions, action=\"add\" ) export_table Description Exports a BigQuery table to a specified format (e.g., CSV, JSON) to Cloud Storage.\nParameters dataset_id (str): The ID of the dataset. table_id (str): The ID of the table. destination (str): The destination path (e.g., gs://bucket_name/file). output_file_format (OutputFileFormat): The format of the exported file (CSV, JSON, or AVRO). compression (Literal): Compression type (GZIP, DEFLATE, SNAPPY, or NONE). Example client.export_table( dataset_id=\"my_dataset\", table_id=\"my_table\", destination=\"gs://my_bucket/my_table.csv\", output_file_format=\"CSV\", compression=\"GZIP\" ) "},"title":"Bigquery"},"/bigquery-utils/modules/datatransfer/":{"data":{"":"","class-initialization#Class Initialization":"","introduction#Introduction":"","methods-overview#Methods Overview":"Introduction This module provides a custom implementation of the DataTransferServiceClient to extend its functionality. It allows advanced querying of scheduled transfers in BigQuery, adding features like caching and filtering by owner email or table.\nClass Initialization The DataTransferClient extends the original Google Cloud DataTransferServiceClient to provide additional utility methods for managing scheduled queries in BigQuery. The enhancements include caching, advanced filtering, and integration with string utilities.\nParameters credentials (str): The user‚Äôs credentials for authentication. client_options (Optional): A dictionary of custom client settings. Example DataTransferClient( credentials: Optional[Credentials] = None, client_options: Optional[Union[ClientOptions, Dict]] = None, def __init__(self, credentials: Optional[Credentials] = None, client_options: Optional[dict] = None) -\u003e None ) Methods Overview list_transfer_configs Description Retrieves all scheduled queries in a project. Optionally, fetches the email of the owner for each transfer configuration.\nParameters dataset_id (str): The ID of the dataset to check. table_id (str): The ID of the table to check. request Optional: A ListTransferConfigsRequest object or equivalent dictionary. parent Optional: The project and location in the format projects/{project_id}/locations/{location_id}. retry Optional: Retry configuration for handling errors. timeout Optional: Timeout for the request. metadata Optional: Additional metadata for the request. with_email Optional: Whether to fetch the email of the transfer config owner (default is False). Example configs = client.list_transfer_configs(parent=\"projects/my-project/locations/my-location\") list_transfer_config_by_owner_email Description Filters transfer configurations by the email of the owner.\nParameters owner_email: The email of the owner to filter by. parent: The project and location in the format projects/{project_id}/locations/{location_id}. Example configs = client.list_transfer_config_by_owner_email( owner_email=\"user@example.com\", parent=\"projects/my-project/locations/us\" ) list_transfer_configs_by_table Description Filters transfer configurations by the table referenced in their queries.\nParameters table_id (str): The name of the table to filter by (not the full path). Returns parent: The project and location in the format projects/{project_id}/locations/{location_id}. Example configs = client.list_transfer_configs_by_table( table_id=\"my_table\", parent=\"projects/my-project/locations/us\" ) "},"title":"Datatransfer"},"/bigquery-utils/modules/utils/constants/":{"data":{"":"","constants#Constants":"","dependencies#Dependencies":"","example-usage#Example Usage":"","notes#Notes":"Overview This module defines the constants used throughout the project. It includes general-purpose constants, logging message templates, and configuration values for integration with Google BigQuery. The constants are designed to ensure consistency and reduce hardcoding across the codebase.\nConstants General Constants DEFAULT_LOG_LEVEL: \"DEBUG\"\nDefault log level for the application, used to control the verbosity of logs. Logging Messages These constants define standard logging messages used throughout the project:\nLOG_SUCCESS: \"SUCCESS\"\nMessage indicating successful operations. LOG_FAILED: \"FAILED\"\nMessage indicating failed operations. LOG_UNHANDLED_EXCEPTION: \"UNHANDLED EXCEPTION\"\nMessage used for unhandled exceptions in the application. BigQuery-Specific Constants These constants relate to Google BigQuery configurations:\nIAM_TO_DATASET_ROLES: A dictionary mapping IAM roles to BigQuery dataset roles: { \"roles/bigquery.dataViewer\": \"READER\", \"roles/bigquery.dataEditor\": \"WRITER\", \"roles/bigquery.dataOwner\": \"OWNER\", } Used for role-based access control (RBAC) in BigQuery datasets.\nOUTPUT_FILE_FORMAT: A dictionary mapping file format names to SourceFormat constants in BigQuery: { \"CSV\": SourceFormat.CSV, \"JSON\": SourceFormat.NEWLINE_DELIMITED_JSON, \"AVRO\": SourceFormat.AVRO, } Specifies the formats for output files in BigQuery export operations.\nType Literals These type literals define specific sets of values to ensure type safety in various operations:\nPartitionTimeGranularities: Allowed values for partition time granularity in BigQuery tables: Literal[\"HOUR\", \"DAY\", \"MONTH\", \"YEAR\"] OutputFileFormat: Allowed output file formats for BigQuery export operations: Literal[\"CSV\", \"JSON\", \"AVRO\"] PermissionActionTypes: Allowed action types for permission management: Literal[\"ADD\", \"REMOVE\", \"UPDATE\"] Example Usage Using Logging Messages import logging from constants import LOG_SUCCESS, LOG_FAILED def log_operation_success(): logging.info(\"Job %s loaded!\",LOG_SUCCESS) def log_operation_failure(): logging.error(\"Problem iwth the job [%s]\", LOG_FAILED) Mapping IAM Roles from constants import IAM_TO_DATASET_ROLES def get_dataset_role(iam_role): return IAM_TO_DATASET_ROLES.get(iam_role, \"UNKNOWN\") Setting Output Formats from constants import OUTPUT_FILE_FORMAT def export_data(file_format): if file_format in OUTPUT_FILE_FORMAT: return OUTPUT_FILE_FORMAT[file_format] else: raise ValueError(\"Invalid file format\") Dependencies google-cloud-bigquery: Provides the SourceFormat class for file formats in BigQuery. Notes This module is designed to centralize project constants to improve maintainability and reduce the risk of errors due to hardcoded values.","overview#Overview":""},"title":"Constants"},"/bigquery-utils/modules/utils/custom-data-checks/":{"data":{"":"","class-initialization#Class Initialization":"","introduction#Introduction":"","methods-overview#Methods Overview":"Introduction This module provides a comprehensive set of functions to validate and check the integrity of data in tabular formats such as CSVs. These checks help ensure the correctness of data by validating rows, columns, and specific values against given criteria.\nKey Features Column Validation: Ensures uniform row lengths and column existence. Uniqueness Check: Detects duplicate values in specified columns. Null Checks: Validates that specified columns do not contain null values. Numeric Range Validation: Confirms numeric values fall within a specified range. String Pattern Matching: Verifies strings match a specified regex pattern. Date Format Validation: Ensures dates adhere to a given format. Datatype Validation: Confirms values match the expected datatype. Value Set Check: Ensures values exist within a specified set. Class Initialization To use the CustomDataChecks utilities, you first need to create an instance of the class:\nfrom bigquery_advanced_utils.utils.custom_data_checks import CustomDataChecks custom_data_checks_util = CustomDataChecks() The CustomDataChecks class does not require any parameters during initialization. Once instantiated, you can call its methods for various string manipulation tasks.\nMethods Overview check_columns Validates that all rows in a dataset have the same number of columns as the header.\nParameters idx (int): Row number. row (list): List of values in the row. header (list): List of column names. column_sums (list): (Unused) Placeholder for memory storage. Raises ValueError: If row lengths do not match the header length. check_unique Ensures specified columns contain unique values across all rows.\nParameters idx (int): Row number. row (list): List of values in the row. header (list): List of column names. column_sums (list): List of sets for tracking seen values per column. columns_to_test (list, optional): Columns to validate for uniqueness. Raises ValueError: If duplicate values are detected. check_no_nulls Validates that specified columns do not contain null or empty values.\nParameters idx (int): Row number. row (list): List of values in the row. header (list): List of column names. column_sums (list): (Unused) Placeholder for memory storage. columns_to_test (list, optional): Columns to check for null values. Raises ValueError: If null values are found. check_numeric_range Ensures numeric values in specified columns fall within a defined range. Allows null values.\nParameters idx (int): Row number. row (list): List of values in the row. header (list): List of column names. column_sums (list): (Unused) Placeholder for memory storage. columns_to_test (list, optional): Columns to check for numeric range. min_value (int|float, optional): Minimum allowed value. max_value (int|float, optional): Maximum allowed value. Raises ValueError: If values fall outside the specified range. check_string_pattern Validates that values in specified columns match a given regex pattern. Allows null values.\nParameters idx (int): Row number. row (list): List of values in the row. header (list): List of column names. column_sums (list): (Unused) Placeholder for memory storage. columns_to_test (list, optional): Columns to validate. regex_pattern (str): Regular expression to match. Raises ValueError: If values do not match the regex pattern. check_date_format Ensures dates in specified columns match a given date format. Allows null values.\nParameters idx (int): Row number. row (list): List of values in the row. header (list): List of column names. column_sums (list): (Unused) Placeholder for memory storage. columns_to_test (list, optional): Columns to validate. date_format (str): Expected date format (default: %Y-%m-%d). Raises ValueError: If values do not match the date format. check_datatype Validates that values in specified columns match the expected datatype. Allows null values.\nParameters idx (int): Row number. row (list): List of values in the row. header (list): List of column names. column_sums (list): (Unused) Placeholder for memory storage. columns_to_test (list, optional): Columns to validate. expected_datatype (type): Expected datatype (e.g., int, float). Raises ValueError: If values do not match the expected datatype. check_in_set Ensures values in specified columns are within a predefined set.\nParameters idx (int): Row number. row (list): List of values in the row. header (list): List of column names. column_sums (list): (Unused) Placeholder for memory storage. columns_to_test (list, optional): Columns to validate. valid_values_set (list): Allowed values for validation. Raises ValueError: If values are not in the allowed set. "},"title":"Custom Data Checks"},"/bigquery-utils/modules/utils/numeric/":{"data":{"":"","class-initialization#Class Initialization":"","introduction#Introduction":"","methods-overview#Methods Overview":"Introduction This module provides a set of utility functions for numeric variables. The Numeric class simplifies operations involving numeric conversions, particularly in the context of byte size transformations into human-readable units.\nClass Initialization To use the Numeric utilities, you need to create an instance of the class:\nnumeric_utils = Numeric() The Numeric class does not require any parameters during initialization. Once instantiated, you can call its methods for various numeric conversions.\nMethods Overview convert_bytes_to_unit Description Converts a number of bytes into a specified unit (e.g., KB, MB, GB, TB).\nParameters byte_count (int): The number of bytes to convert. unit (str): The target unit. Supported values are \"KB\", \"MB\", \"GB\", \"TB\". Returns float: The byte count converted to the specified unit. Raises ValueError: If an unsupported unit is provided. Example numeric_utils = Numeric() try: size_in_gb = numeric_utils.convert_bytes_to_unit(1073741824, \"GB\") print(size_in_gb) # Output: 1.0 except ValueError as e: print(f\"Error: {e}\") "},"title":"Numeric"},"/bigquery-utils/modules/utils/string/":{"data":{"":"","class-initialization#Class Initialization":"","introduction#Introduction":"","methods-overview#Methods Overview":"Introduction This Python module provides a set of utility functions to manipulate strings, making it easier to handle text processing tasks such as cleaning strings, removing comments, and extracting tables from queries. These utilities are part of the bigquery-advanced-utils package and aim to simplify text processing in data workflows.\nClass Initialization To use the String utilities, you first need to create an instance of the class:\nfrom bigquery_advanced_utils.utils.string import String string_utils = String() The String class does not require any parameters during initialization. Once instantiated, you can call its methods for various string manipulation tasks.\nMethods Overview remove_chars_from_string Description Removes specific characters from a given string.\nParameters string (str): The input string to process. chars_to_remove (list[str]): A list of characters to remove from the input string. Returns str: A new string with the specified characters removed. Raises InvalidArgumentToFunction: If the input parameters are invalid (e.g., string is None, or chars_to_remove is not a non-empty list). Example from bigquery_advanced_utils.utils.string import String string_utils = String() input_string = \"Hello, World!\" chars_to_remove = [\",\", \"!\"] try: cleaned_string = string_utils.remove_chars_from_string(input_string, chars_to_remove) print(cleaned_string) # Output: \"Hello World\" except InvalidArgumentToFunction: print(\"Invalid arguments provided to the function.\") remove_comments_from_string Description Removes all comments from a given string based on the specified dialect (default is standard_sql).\nParameters string (str): The input string, typically a SQL query. dialect (str): The dialect to use for identifying comments. Default is standard_sql. Returns str: The string with all comments removed. Raises InvalidArgumentToFunction: If the input parameters are invalid (e.g., string is None). Example input_query = \"\"\"SELECT * FROM table -- This is a comment WHERE id = 1;\"\"\" try: cleaned_query = string_utils.remove_comments_from_string(input_query) print(cleaned_query) # Output: \"SELECT * FROM table WHERE id = 1;\" except InvalidArgumentToFunction: print(\"Invalid arguments provided to the function.\") extract_tables_from_query Description Extracts all table names from a SQL query string.\nParameters string (str): The input SQL query string. Returns list[str]: A list of unique table names extracted from the query. Raises InvalidArgumentToFunction: If the input string is None. Example: query = \"\"\"SELECT * FROM `project.dataset.table` JOIN `project.dataset.table2` ON id = id -- Commented out code WHERE condition;\"\"\" try: tables = string_utils.extract_tables_from_query(query) print(tables) # Output: [\"project.dataset.table\", \"project.dataset.table2\"] except InvalidArgumentToFunction: print(\"Invalid arguments provided to the function.\") "},"title":"String"},"/blog/":{"data":{"":"\rRSS Feed "},"title":"Blog"},"/blog/reset-git-history/":{"data":{"":"","benefits-of-resetting-a-repository#Benefits of Resetting a Repository":"Learn how to reset your Git repository by creating a new main branch with no previous history. This step-by-step guide walks you through creating an orphan branch, removing the old main branch, and force-updating your remote repository.\nIntroduction When working with Git, there might be times when you want to reset your repository completely, erasing its history and starting over with a clean slate. This tutorial will guide you through the process of creating a new main branch and replacing the old one, effectively resetting your repository.\nThis process is destructive and will permanently delete the history of your repository, so proceed with caution and make backups if needed.\nWhy reset a Repository? Start Fresh: Begin with a clean history for a new phase of development. Reduce Repository Size: Remove unnecessary historical data. Fix Mistakes: Eliminate sensitive information or erroneous commits. Simplify History: Clean up a messy commit history for easier navigation. Steps to Reset a Git Repository Follow these steps to reset your repository:\nCreate an Orphan Branch An orphan branch is not connected to any previous commits in the repository.\ngit checkout --orphan latest_branch This creates a new branch named latest_branch with no history.\nAdd All Files You need to stage all files for the initial commit in the orphan branch.\ngit add -A This command stages all files, including new and modified ones.\nCommit the Changes Create a new commit in the orphan branch.\ngit commit -am \"Initial commit for the new main branch\" This becomes the first commit in your new branch.\nDelete the Old Main Branch Once the orphan branch is ready, delete the old main branch.\ngit branch -D main ‚ö†Ô∏è Warning: This action is permanent and cannot be undone.\nRename the New Branch to Main Rename the orphan branch to main to make it the primary branch.\ngit branch -m main Force Push to Update the Remote Repository To apply the changes to the remote repository, you must force push the new branch.\ngit push -f origin main The -f (force) option is required because you are replacing the existing remote history.\nImportant Notes Destructive Operation: Resetting the repository will permanently delete all previous commits. Make backups if necessary. Collaborators: Inform collaborators beforehand, as their local copies will no longer match the remote. They will need to re-clone the repository after this process. Use Cases: Only use this process when absolutely necessary, such as removing sensitive data, cleaning up large repositories, or starting a fresh phase of a project. Benefits of Resetting a Repository Reduced Complexity: A cleaner history for easier navigation and understanding. Optimized Size: Removes unnecessary files and commits, reducing repository size. Security: Eliminates sensitive or private information from version history. With this process, you can start fresh with a brand-new main branch while eliminating the previous history. This is a powerful Git feature but should be used carefully due to its irreversible nature.","important-notes#Important Notes":"","introduction#Introduction":"","steps-to-reset-a-git-repository#Steps to Reset a Git Repository":"","why-reset-a-repository#Why reset a Repository?":""},"title":"How to Completely Reset a Git Repository: Starting Fresh with a New Main Branch"}}